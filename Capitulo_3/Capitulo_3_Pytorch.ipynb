{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 3: Pytorch Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=SummaryWriter()\n",
    "funcs={\"sin\":math.sin,\"cos\":math.cos,\"tan\":math.tan}\n",
    "for angle in range(-360,360):\n",
    "    angle_rad=angle*math.pi/180\n",
    "    for name,func in funcs.items():\n",
    "        val=func(angle_rad)\n",
    "        writer.add_scalar(name,val,angle)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN over an Atari game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# dimension input image will be rescaled\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n",
    "\n",
    "\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # resize image\n",
    "        new_obs = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        # transform (210, 160, 3) -> (3, 210, 160)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "    batch = [e.reset() for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True:\n",
    "        e = next(env_gen)\n",
    "        obs, reward, is_done, _ = e.step(e.action_space.sample())\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32) * 2.0 / 255.0 - 1.0\n",
    "            yield torch.tensor(batch_np)\n",
    "            batch.clear()\n",
    "        if is_done:\n",
    "            e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Breakout-v0\n",
      "INFO: Making new env: AirRaid-v0\n",
      "INFO: Making new env: Pong-v0\n",
      "INFO: Iter 100: gen_loss=5.383e+00, dis_loss=5.300e-02\n",
      "INFO: Iter 200: gen_loss=7.094e+00, dis_loss=5.213e-03\n",
      "INFO: Iter 300: gen_loss=7.502e+00, dis_loss=2.094e-03\n",
      "INFO: Iter 400: gen_loss=7.871e+00, dis_loss=1.143e-03\n",
      "INFO: Iter 500: gen_loss=8.469e+00, dis_loss=9.071e-02\n",
      "INFO: Iter 600: gen_loss=8.221e+00, dis_loss=1.218e-02\n",
      "INFO: Iter 700: gen_loss=8.069e+00, dis_loss=5.963e-03\n",
      "INFO: Iter 800: gen_loss=7.291e+00, dis_loss=1.246e-01\n",
      "INFO: Iter 900: gen_loss=6.306e+00, dis_loss=7.812e-03\n",
      "INFO: Iter 1000: gen_loss=6.270e+00, dis_loss=1.052e-02\n",
      "INFO: Iter 1100: gen_loss=7.734e+00, dis_loss=8.881e-02\n",
      "INFO: Iter 1200: gen_loss=5.545e+00, dis_loss=2.673e-01\n",
      "INFO: Iter 1300: gen_loss=5.327e+00, dis_loss=3.105e-01\n",
      "INFO: Iter 1400: gen_loss=4.497e+00, dis_loss=2.723e-01\n",
      "INFO: Iter 1500: gen_loss=4.356e+00, dis_loss=2.351e-01\n",
      "INFO: Iter 1600: gen_loss=5.075e+00, dis_loss=1.433e-01\n",
      "INFO: Iter 1700: gen_loss=5.408e+00, dis_loss=1.424e-01\n",
      "INFO: Iter 1800: gen_loss=6.977e+00, dis_loss=1.164e-02\n",
      "INFO: Iter 1900: gen_loss=5.048e+00, dis_loss=1.994e-01\n",
      "INFO: Iter 2000: gen_loss=5.108e+00, dis_loss=3.038e-01\n",
      "INFO: Iter 2100: gen_loss=5.189e+00, dis_loss=1.995e-01\n",
      "INFO: Iter 2200: gen_loss=5.948e+00, dis_loss=1.186e-01\n",
      "INFO: Iter 2300: gen_loss=5.835e+00, dis_loss=1.785e-01\n",
      "INFO: Iter 2400: gen_loss=5.330e+00, dis_loss=1.875e-01\n",
      "INFO: Iter 2500: gen_loss=5.736e+00, dis_loss=5.068e-02\n",
      "INFO: Iter 2600: gen_loss=6.268e+00, dis_loss=1.419e-02\n",
      "INFO: Iter 2700: gen_loss=6.216e+00, dis_loss=1.776e-02\n",
      "INFO: Iter 2800: gen_loss=6.576e+00, dis_loss=1.283e-01\n",
      "INFO: Iter 2900: gen_loss=6.028e+00, dis_loss=5.896e-02\n",
      "INFO: Iter 3000: gen_loss=7.264e+00, dis_loss=2.033e-02\n",
      "INFO: Iter 3100: gen_loss=5.578e+00, dis_loss=2.158e-01\n",
      "INFO: Iter 3200: gen_loss=6.182e+00, dis_loss=1.490e-01\n",
      "INFO: Iter 3300: gen_loss=7.102e+00, dis_loss=1.485e-02\n",
      "INFO: Iter 3400: gen_loss=7.861e+00, dis_loss=6.406e-03\n",
      "INFO: Iter 3500: gen_loss=6.550e+00, dis_loss=2.277e-01\n",
      "INFO: Iter 3600: gen_loss=6.507e+00, dis_loss=7.443e-02\n",
      "INFO: Iter 3700: gen_loss=6.735e+00, dis_loss=4.579e-02\n",
      "INFO: Iter 3800: gen_loss=7.749e+00, dis_loss=6.946e-03\n",
      "INFO: Iter 3900: gen_loss=5.473e+00, dis_loss=3.122e-01\n",
      "INFO: Iter 4000: gen_loss=6.300e+00, dis_loss=3.328e-02\n",
      "INFO: Iter 4100: gen_loss=6.108e+00, dis_loss=2.150e-01\n",
      "INFO: Iter 4200: gen_loss=6.824e+00, dis_loss=8.384e-03\n",
      "INFO: Iter 4300: gen_loss=7.172e+00, dis_loss=1.149e-02\n",
      "INFO: Iter 4400: gen_loss=8.131e+00, dis_loss=4.356e-03\n",
      "INFO: Iter 4500: gen_loss=8.293e+00, dis_loss=8.612e-03\n",
      "INFO: Iter 4600: gen_loss=8.155e+00, dis_loss=9.620e-03\n",
      "INFO: Iter 4700: gen_loss=5.952e+00, dis_loss=2.559e-01\n",
      "INFO: Iter 4800: gen_loss=6.958e+00, dis_loss=2.618e-02\n",
      "INFO: Iter 4900: gen_loss=7.888e+00, dis_loss=8.022e-03\n",
      "INFO: Iter 5000: gen_loss=5.642e+00, dis_loss=2.600e-01\n",
      "INFO: Iter 5100: gen_loss=6.797e+00, dis_loss=1.131e-02\n",
      "INFO: Iter 5200: gen_loss=6.769e+00, dis_loss=1.070e-01\n",
      "INFO: Iter 5300: gen_loss=7.100e+00, dis_loss=5.010e-03\n",
      "INFO: Iter 5400: gen_loss=7.528e+00, dis_loss=4.752e-03\n",
      "INFO: Iter 5500: gen_loss=7.010e+00, dis_loss=1.425e-01\n",
      "INFO: Iter 5600: gen_loss=7.659e+00, dis_loss=1.144e-02\n",
      "INFO: Iter 5700: gen_loss=5.086e+00, dis_loss=3.550e-01\n",
      "INFO: Iter 5800: gen_loss=6.342e+00, dis_loss=2.274e-01\n",
      "INFO: Iter 5900: gen_loss=7.625e+00, dis_loss=4.506e-02\n",
      "INFO: Iter 6000: gen_loss=7.675e+00, dis_loss=3.857e-02\n",
      "INFO: Iter 6100: gen_loss=7.425e+00, dis_loss=9.747e-02\n",
      "INFO: Iter 6200: gen_loss=8.047e+00, dis_loss=1.594e-02\n",
      "INFO: Iter 6300: gen_loss=9.055e+00, dis_loss=4.398e-03\n",
      "INFO: Iter 6400: gen_loss=7.690e+00, dis_loss=1.470e-01\n",
      "INFO: Iter 6500: gen_loss=6.443e+00, dis_loss=1.304e-01\n",
      "INFO: Iter 6600: gen_loss=7.053e+00, dis_loss=2.729e-02\n",
      "INFO: Iter 6700: gen_loss=7.492e+00, dis_loss=4.357e-02\n",
      "INFO: Iter 6800: gen_loss=7.871e+00, dis_loss=1.141e-01\n",
      "INFO: Iter 6900: gen_loss=6.111e+00, dis_loss=1.564e-01\n",
      "INFO: Iter 7000: gen_loss=6.871e+00, dis_loss=7.461e-02\n",
      "INFO: Iter 7100: gen_loss=7.368e+00, dis_loss=7.213e-03\n",
      "INFO: Iter 7200: gen_loss=8.026e+00, dis_loss=3.660e-02\n",
      "INFO: Iter 7300: gen_loss=8.670e+00, dis_loss=3.547e-03\n",
      "INFO: Iter 7400: gen_loss=8.749e+00, dis_loss=2.633e-03\n",
      "INFO: Iter 7500: gen_loss=8.419e+00, dis_loss=1.432e-02\n",
      "INFO: Iter 7600: gen_loss=9.512e+00, dis_loss=6.505e-03\n",
      "INFO: Iter 7700: gen_loss=7.441e+00, dis_loss=1.233e-01\n",
      "INFO: Iter 7800: gen_loss=7.297e+00, dis_loss=3.432e-02\n",
      "INFO: Iter 7900: gen_loss=7.414e+00, dis_loss=1.428e-01\n",
      "INFO: Iter 8000: gen_loss=7.669e+00, dis_loss=7.601e-02\n",
      "INFO: Iter 8100: gen_loss=8.228e+00, dis_loss=8.378e-02\n",
      "INFO: Iter 8200: gen_loss=7.092e+00, dis_loss=9.554e-02\n",
      "INFO: Iter 8300: gen_loss=7.809e+00, dis_loss=5.025e-03\n",
      "INFO: Iter 8400: gen_loss=9.052e+00, dis_loss=1.023e-02\n",
      "INFO: Iter 8500: gen_loss=9.227e+00, dis_loss=1.104e-02\n",
      "INFO: Iter 8600: gen_loss=8.562e+00, dis_loss=1.224e-01\n",
      "INFO: Iter 8700: gen_loss=8.245e+00, dis_loss=1.028e-02\n",
      "INFO: Iter 8800: gen_loss=7.683e+00, dis_loss=1.210e-01\n",
      "INFO: Iter 8900: gen_loss=7.657e+00, dis_loss=5.578e-02\n",
      "INFO: Iter 9000: gen_loss=7.122e+00, dis_loss=9.120e-02\n",
      "INFO: Iter 9100: gen_loss=7.349e+00, dis_loss=4.940e-02\n",
      "INFO: Iter 9200: gen_loss=8.064e+00, dis_loss=6.247e-03\n",
      "INFO: Iter 9300: gen_loss=9.216e+00, dis_loss=3.051e-03\n",
      "INFO: Iter 9400: gen_loss=8.972e+00, dis_loss=6.038e-02\n",
      "INFO: Iter 9500: gen_loss=6.951e+00, dis_loss=2.714e-01\n",
      "INFO: Iter 9600: gen_loss=7.987e+00, dis_loss=1.103e-01\n",
      "INFO: Iter 9700: gen_loss=9.001e+00, dis_loss=7.981e-02\n",
      "INFO: Iter 9800: gen_loss=8.909e+00, dis_loss=9.811e-02\n",
      "INFO: Iter 9900: gen_loss=8.341e+00, dis_loss=1.116e-01\n",
      "INFO: Iter 10000: gen_loss=8.464e+00, dis_loss=6.710e-03\n",
      "INFO: Iter 10100: gen_loss=9.431e+00, dis_loss=4.822e-03\n",
      "INFO: Iter 10200: gen_loss=6.717e+00, dis_loss=2.863e-01\n",
      "INFO: Iter 10300: gen_loss=5.836e+00, dis_loss=4.963e-02\n",
      "INFO: Iter 10400: gen_loss=6.668e+00, dis_loss=3.659e-01\n",
      "INFO: Iter 10500: gen_loss=5.091e+00, dis_loss=3.406e-01\n",
      "INFO: Iter 10600: gen_loss=5.238e+00, dis_loss=1.422e-01\n",
      "INFO: Iter 10700: gen_loss=6.057e+00, dis_loss=1.221e-01\n",
      "INFO: Iter 10800: gen_loss=5.659e+00, dis_loss=2.555e-01\n",
      "INFO: Iter 10900: gen_loss=7.128e+00, dis_loss=4.415e-02\n",
      "INFO: Iter 11000: gen_loss=7.820e+00, dis_loss=3.800e-02\n",
      "INFO: Iter 11100: gen_loss=8.788e+00, dis_loss=9.297e-03\n",
      "INFO: Iter 11200: gen_loss=8.414e+00, dis_loss=3.640e-03\n",
      "INFO: Iter 11300: gen_loss=9.274e+00, dis_loss=6.062e-03\n",
      "INFO: Iter 11400: gen_loss=9.006e+00, dis_loss=2.506e-01\n",
      "INFO: Iter 11500: gen_loss=5.254e+00, dis_loss=3.777e-01\n",
      "INFO: Iter 11600: gen_loss=5.819e+00, dis_loss=1.820e-01\n",
      "INFO: Iter 11700: gen_loss=5.967e+00, dis_loss=5.387e-02\n",
      "INFO: Iter 11800: gen_loss=7.331e+00, dis_loss=1.059e-02\n",
      "INFO: Iter 11900: gen_loss=8.000e+00, dis_loss=3.717e-02\n",
      "INFO: Iter 12000: gen_loss=7.009e+00, dis_loss=7.367e-02\n",
      "INFO: Iter 12100: gen_loss=5.135e+00, dis_loss=3.153e-01\n",
      "INFO: Iter 12200: gen_loss=6.415e+00, dis_loss=2.060e-02\n",
      "INFO: Iter 12300: gen_loss=7.149e+00, dis_loss=6.575e-03\n",
      "INFO: Iter 12400: gen_loss=8.259e+00, dis_loss=8.083e-03\n",
      "INFO: Iter 12500: gen_loss=8.851e+00, dis_loss=5.720e-02\n",
      "INFO: Iter 12600: gen_loss=4.526e+00, dis_loss=3.676e-01\n",
      "INFO: Iter 12700: gen_loss=5.974e+00, dis_loss=1.741e-01\n",
      "INFO: Iter 12800: gen_loss=5.366e+00, dis_loss=2.487e-01\n",
      "INFO: Iter 12900: gen_loss=5.990e+00, dis_loss=9.942e-02\n",
      "INFO: Iter 13000: gen_loss=6.744e+00, dis_loss=1.188e-02\n",
      "INFO: Iter 13100: gen_loss=6.503e+00, dis_loss=7.331e-03\n",
      "INFO: Iter 13200: gen_loss=7.211e+00, dis_loss=4.979e-03\n",
      "INFO: Iter 13300: gen_loss=8.671e+00, dis_loss=7.032e-03\n",
      "INFO: Iter 13400: gen_loss=5.701e+00, dis_loss=4.722e-01\n",
      "INFO: Iter 13500: gen_loss=4.721e+00, dis_loss=1.584e-01\n",
      "INFO: Iter 13600: gen_loss=6.179e+00, dis_loss=1.681e-01\n",
      "INFO: Iter 13700: gen_loss=5.688e+00, dis_loss=3.067e-02\n",
      "INFO: Iter 13800: gen_loss=6.577e+00, dis_loss=4.286e-03\n",
      "INFO: Iter 13900: gen_loss=6.724e+00, dis_loss=5.449e-03\n",
      "INFO: Iter 14000: gen_loss=5.600e+00, dis_loss=3.803e-01\n",
      "INFO: Iter 14100: gen_loss=3.609e+00, dis_loss=3.755e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-692cca953eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdis_output_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_discr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_output_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgen_loss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_output_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgen_loss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mgen_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = [InputWrapper(gym.make(name)) for name in ('Breakout-v0', 'AirRaid-v0', 'Pong-v0')]\n",
    "input_shape = envs[0].observation_space.shape\n",
    "\n",
    "net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "objective = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "writer = SummaryWriter()\n",
    "\n",
    "gen_losses = []\n",
    "dis_losses = []\n",
    "iter_no = 0\n",
    "\n",
    "true_labels_v = torch.ones(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "fake_labels_v = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "for batch_v in iterate_batches(envs):\n",
    "    # generate extra fake samples, input is 4D: batch, filters, x, y\n",
    "    gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1).normal_(0, 1).to(device)\n",
    "    batch_v = batch_v.to(device)\n",
    "    gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "    # train discriminator\n",
    "    dis_optimizer.zero_grad()\n",
    "    dis_output_true_v = net_discr(batch_v)\n",
    "    dis_output_fake_v = net_discr(gen_output_v.detach())\n",
    "    dis_loss = objective(dis_output_true_v, true_labels_v) + objective(dis_output_fake_v, fake_labels_v)\n",
    "    dis_loss.backward()\n",
    "    dis_optimizer.step()\n",
    "    dis_losses.append(dis_loss.item())\n",
    "\n",
    "    # train generator\n",
    "    gen_optimizer.zero_grad()\n",
    "    dis_output_v = net_discr(gen_output_v)\n",
    "    gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "    gen_loss_v.backward()\n",
    "    gen_optimizer.step()\n",
    "    gen_losses.append(gen_loss_v.item())\n",
    "\n",
    "    iter_no += 1\n",
    "    if iter_no % REPORT_EVERY_ITER == 0:\n",
    "        log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", iter_no, np.mean(gen_losses), np.mean(dis_losses))\n",
    "        writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "        writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "        gen_losses = []\n",
    "        dis_losses = []\n",
    "    if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "        writer.add_image(\"fake\", vutils.make_grid(gen_output_v.data[:64], normalize=True), iter_no)\n",
    "        writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64], normalize=True), iter_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
