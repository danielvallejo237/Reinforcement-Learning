{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functioning-angle",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo de reinforce en el caso discreto y continuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-joseph",
   "metadata": {},
   "source": [
    "# Caso discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "certified-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import pdb\n",
    "from torch.autograd import Variable\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-documentary",
   "metadata": {},
   "source": [
    "El primer paso es construir la red de política o policy network, generalmente consta de capas densas en donde se da la dimensión de entrada y la dimesión de salida de la red, calculan probabilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "boxed-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(nn.Module):\n",
    "    def __init__(self,hidden_size,num_inputs,action_space):\n",
    "        super(Pi,self).__init__()\n",
    "        self.action_space=action_space\n",
    "        out_dim=action_space.n\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_inputs=num_inputs\n",
    "        layers=[\n",
    "            nn.Linear(self.num_inputs,self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size,out_dim),\n",
    "            nn.Softmax(),  \n",
    "        ]\n",
    "        '''\n",
    "        En el modelo descrito en el libro usan Categorical para sacar las probailidades de las\n",
    "        acciones, en este caso usamos la función Soft Max\n",
    "        Pi en este caso representa nuestra Policy Network\n",
    "        '''\n",
    "        self.model=nn.Sequential(*layers)\n",
    "        self.train()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "unique-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self,action_space,hidden_size,num_inputs):\n",
    "        self.action_space=action_space\n",
    "        self.model=Pi(hidden_size,num_inputs,action_space)\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model=self.model.to(self.device) #Mandamos el modelo a cuda para entrenar\n",
    "        self.model.train()\n",
    "        self.optimizer=optim.Adam(self.model.parameters(),lr=1e-3) #Estamos optimizando el peso de la política\n",
    "        #Usamos como optimizador descenso de gradiente estocástico\n",
    "    def select_action(self,state):\n",
    "        probs=self.model(Variable(state).to(self.device))\n",
    "        action=probs.multinomial(1).data\n",
    "        prob=probs[:,action[0,0]].view(1,-1)\n",
    "        log_prob=prob.log()\n",
    "        entropy=-(probs*probs.log()).sum()\n",
    "        return action[0], log_prob, entropy\n",
    "    def update_parameters(self,rewards,log_probs,entropy,gamma):\n",
    "        '''\n",
    "        En este caso queremos actualizar el Policy Gradient el cual necesita de las recompesas, la log\n",
    "        probabilidad y el factor de descuento gamma\n",
    "        '''\n",
    "        R=torch.zeros(1,1)\n",
    "        loss=0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R=rewards[t]+gamma*R\n",
    "            loss=loss- (log_probs[t]*(Variable(R).expand_as(log_probs[t])).to(self.device)).sum()-(0.00001*entropy[t].to(self.device)).sum()\n",
    "        loss=loss/len(rewards)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-victoria",
   "metadata": {},
   "source": [
    "Recordemos lo siguiente:\n",
    "\n",
    "El \"Policy gradient\", se define como $\\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\sum_{t=0}^{T}R_{t}(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$\n",
    "Sin embargo usando la actualización por muestreo de Monte Carlo, la actualización toma la siguiete forma\n",
    "$$ \\nabla_{\\theta} J(\\pi_{\\theta})= \\nabla_{\\theta} J(\\pi_{\\theta})+ R(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$$ para algun tiempo $t \\in \\{0,...,T\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-means",
   "metadata": {},
   "source": [
    "# Entrenamiento principal en ambientes de gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "pleased-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"CartPole-v0\"\n",
    "gamma=0.99\n",
    "hidden_size=128\n",
    "env=gym.make(env_name)\n",
    "#env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "#Lo anterior para llevar un monitoreo del entrenamiento de las cosas\n",
    "num_episodes=1000\n",
    "num_steps=200\n",
    "agent=REINFORCE(env.action_space,hidden_size,env.observation_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "political-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "gothic-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-135-864ce215360d>:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  utils.clip_grad_norm(self.model.parameters(),40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 11.0 Solved: False\n",
      "Episode: 10 Reward: 16.0 Solved: False\n",
      "Episode: 20 Reward: 24.0 Solved: False\n",
      "Episode: 30 Reward: 41.0 Solved: False\n",
      "Episode: 40 Reward: 27.0 Solved: False\n",
      "Episode: 50 Reward: 23.0 Solved: False\n",
      "Episode: 60 Reward: 32.0 Solved: False\n",
      "Episode: 70 Reward: 78.0 Solved: False\n",
      "Episode: 80 Reward: 25.0 Solved: False\n",
      "Episode: 90 Reward: 34.0 Solved: False\n",
      "Episode: 100 Reward: 25.0 Solved: False\n",
      "Episode: 110 Reward: 30.0 Solved: False\n",
      "Episode: 120 Reward: 40.0 Solved: False\n",
      "Episode: 130 Reward: 38.0 Solved: False\n",
      "Episode: 140 Reward: 55.0 Solved: False\n",
      "Episode: 150 Reward: 24.0 Solved: False\n",
      "Episode: 160 Reward: 65.0 Solved: False\n",
      "Episode: 170 Reward: 58.0 Solved: False\n",
      "Episode: 180 Reward: 32.0 Solved: False\n",
      "Episode: 190 Reward: 47.0 Solved: False\n",
      "Episode: 200 Reward: 38.0 Solved: False\n",
      "Episode: 210 Reward: 9.0 Solved: False\n",
      "Episode: 220 Reward: 39.0 Solved: False\n",
      "Episode: 230 Reward: 82.0 Solved: False\n",
      "Episode: 240 Reward: 20.0 Solved: False\n",
      "Episode: 250 Reward: 21.0 Solved: False\n",
      "Episode: 260 Reward: 72.0 Solved: False\n",
      "Episode: 270 Reward: 68.0 Solved: False\n",
      "Episode: 280 Reward: 11.0 Solved: False\n",
      "Episode: 290 Reward: 48.0 Solved: False\n",
      "Episode: 300 Reward: 51.0 Solved: False\n",
      "Episode: 310 Reward: 53.0 Solved: False\n",
      "Episode: 320 Reward: 51.0 Solved: False\n",
      "Solved after 325 episodes\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state=torch.Tensor([env.reset()])\n",
    "    log_probs=[]\n",
    "    entropys=[]\n",
    "    rewards=[]\n",
    "    for t in range(num_steps):\n",
    "        action,log_prob,entropy=agent.select_action(state)\n",
    "        action=action.cpu()\n",
    "        next_state,reward,done,_=env.step(action.numpy()[0]) #El step necesita una acción para llegar al siguiente estado y sacar recompensa\n",
    "        #Actualizamos la entropia y las recompensas y las log probs\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropys.append(entropy)\n",
    "        state=torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_parameters(rewards,log_probs,entropys,gamma)\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,np.sum(rewards),(np.sum(rewards)>195.0)))\n",
    "    if np.sum(rewards)>195.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-circular",
   "metadata": {},
   "source": [
    "# Caso del reinforce continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "living-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def _action(self,action):\n",
    "        action=(action+1)/2\n",
    "        action*=(self.action_space.high-self.action_space.low)\n",
    "        action+=self.action_space.low\n",
    "        return action\n",
    "    def _reverse_action(self,action):\n",
    "        '''\n",
    "        Regresar la acción al estado original en el que se encontraba\n",
    "        '''\n",
    "        action-=self.action_space.low\n",
    "        action/=(self.action_space.high-self.action_space.low)\n",
    "        action=2*action-1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "    self.model = self.model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).cuda())\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "\n",
    "        eps = torch.randn(mu.size())\n",
    "        # Cálculo de la probabilidad\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
    "\n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).cuda()).sum() - (0.0001*entropies[i].cuda()).sum()\n",
    "        loss = loss / len(rewards)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
