{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functioning-angle",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo de reinforce en el caso discreto y continuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-joseph",
   "metadata": {},
   "source": [
    "# Caso discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "certified-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import pdb\n",
    "from torch.autograd import Variable\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-documentary",
   "metadata": {},
   "source": [
    "El primer paso es construir la red de política o policy network, generalmente consta de capas densas en donde se da la dimensión de entrada y la dimesión de salida de la red, calculan probabilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boxed-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(nn.Module):\n",
    "    def __init__(self,hidden_size,num_inputs,action_space):\n",
    "        super(Pi,self).__init__()\n",
    "        self.action_space=action_space\n",
    "        out_dim=action_space.n\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_inputs=num_inputs\n",
    "        layers=[\n",
    "            nn.Linear(self.num_inputs,self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size,out_dim),\n",
    "            nn.Softmax(),  \n",
    "        ]\n",
    "        '''\n",
    "        En el modelo descrito en el libro usan Categorical para sacar las probailidades de las\n",
    "        acciones, en este caso usamos la función Soft Max\n",
    "        Pi en este caso representa nuestra Policy Network\n",
    "        '''\n",
    "        self.model=nn.Sequential(*layers)\n",
    "        self.train()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self,action_space,hidden_size,num_inputs):\n",
    "        self.action_space=action_space\n",
    "        self.model=Pi(hidden_size,num_inputs,action_space)\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model=self.model.to(self.device) #Mandamos el modelo a cuda para entrenar\n",
    "        self.model.train()\n",
    "        self.optimizer=optim.Adam(self.model.parameters(),lr=1e-3) #Estamos optimizando el peso de la política\n",
    "        #Usamos como optimizador descenso de gradiente estocástico\n",
    "    def select_action(self,state):\n",
    "        probs=self.model(Variable(state).to(self.device))\n",
    "        action=probs.multinomial(1).data\n",
    "        prob=probs[:,action[0,0]].view(1,-1)\n",
    "        log_prob=prob.log()\n",
    "        entropy=-(probs*probs.log()).sum()\n",
    "        return action[0], log_prob, entropy\n",
    "    def update_parameters(self,rewards,log_probs,entropy,gamma):\n",
    "        '''\n",
    "        En este caso queremos actualizar el Policy Gradient el cual necesita de las recompesas, la log\n",
    "        probabilidad y el factor de descuento gamma\n",
    "        '''\n",
    "        R=torch.zeros(1,1)\n",
    "        loss=0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R=rewards[t]+gamma*R\n",
    "            loss=loss- (log_probs[t]*(Variable(R).expand_as(log_probs[t])).to(self.device)).sum()-(0.00001*entropy[t].to(self.device)).sum()\n",
    "        loss=loss/len(rewards)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-victoria",
   "metadata": {},
   "source": [
    "Recordemos lo siguiente:\n",
    "\n",
    "El \"Policy gradient\", se define como $\\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\sum_{t=0}^{T}R_{t}(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$\n",
    "Sin embargo usando la actualización por muestreo de Monte Carlo, la actualización toma la siguiete forma\n",
    "$$ \\nabla_{\\theta} J(\\pi_{\\theta})= \\nabla_{\\theta} J(\\pi_{\\theta})+ R(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$$ para algun tiempo $t \\in \\{0,...,T\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-means",
   "metadata": {},
   "source": [
    "# Entrenamiento principal en ambientes de gym, CartPole V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"CartPole-v0\"\n",
    "gamma=0.99\n",
    "hidden_size=128\n",
    "env=gym.make(env_name)\n",
    "#env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "#Lo anterior para llevar un monitoreo del entrenamiento de las cosas\n",
    "num_episodes=1000\n",
    "num_steps=200\n",
    "agent=REINFORCE(env.action_space,hidden_size,env.observation_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "political-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gothic-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 20.0 Solved: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel.vallejo/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "<ipython-input-4-864ce215360d>:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  utils.clip_grad_norm(self.model.parameters(),40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 Reward: 17.0 Solved: False\n",
      "Episode: 20 Reward: 35.0 Solved: False\n",
      "Episode: 30 Reward: 34.0 Solved: False\n",
      "Episode: 40 Reward: 49.0 Solved: False\n",
      "Episode: 50 Reward: 64.0 Solved: False\n",
      "Episode: 60 Reward: 18.0 Solved: False\n",
      "Episode: 70 Reward: 30.0 Solved: False\n",
      "Episode: 80 Reward: 34.0 Solved: False\n",
      "Episode: 90 Reward: 22.0 Solved: False\n",
      "Episode: 100 Reward: 15.0 Solved: False\n",
      "Episode: 110 Reward: 56.0 Solved: False\n",
      "Episode: 120 Reward: 21.0 Solved: False\n",
      "Episode: 130 Reward: 67.0 Solved: False\n",
      "Episode: 140 Reward: 13.0 Solved: False\n",
      "Episode: 150 Reward: 50.0 Solved: False\n",
      "Episode: 160 Reward: 82.0 Solved: False\n",
      "Episode: 170 Reward: 31.0 Solved: False\n",
      "Episode: 180 Reward: 37.0 Solved: False\n",
      "Episode: 190 Reward: 32.0 Solved: False\n",
      "Episode: 200 Reward: 74.0 Solved: False\n",
      "Episode: 210 Reward: 35.0 Solved: False\n",
      "Episode: 220 Reward: 36.0 Solved: False\n",
      "Episode: 230 Reward: 58.0 Solved: False\n",
      "Episode: 240 Reward: 49.0 Solved: False\n",
      "Episode: 250 Reward: 137.0 Solved: False\n",
      "Episode: 260 Reward: 67.0 Solved: False\n",
      "Episode: 270 Reward: 72.0 Solved: False\n",
      "Episode: 280 Reward: 24.0 Solved: False\n",
      "Episode: 290 Reward: 34.0 Solved: False\n",
      "Episode: 300 Reward: 82.0 Solved: False\n",
      "Episode: 310 Reward: 86.0 Solved: False\n",
      "Episode: 320 Reward: 181.0 Solved: False\n",
      "Episode: 330 Reward: 114.0 Solved: False\n",
      "Episode: 340 Reward: 103.0 Solved: False\n",
      "Episode: 350 Reward: 47.0 Solved: False\n",
      "Episode: 360 Reward: 109.0 Solved: False\n",
      "Episode: 370 Reward: 32.0 Solved: False\n",
      "Episode: 380 Reward: 82.0 Solved: False\n",
      "Episode: 390 Reward: 128.0 Solved: False\n",
      "Solved after 391 episodes\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state=torch.Tensor([env.reset()])\n",
    "    log_probs=[]\n",
    "    entropys=[]\n",
    "    rewards=[]\n",
    "    for t in range(num_steps):\n",
    "        action,log_prob,entropy=agent.select_action(state)\n",
    "        action=action.cpu()\n",
    "        next_state,reward,done,_=env.step(action.numpy()[0]) #El step necesita una acción para llegar al siguiente estado y sacar recompensa\n",
    "        #Actualizamos la entropia y las recompensas y las log probs\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropys.append(entropy)\n",
    "        state=torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_parameters(rewards,log_probs,entropys,gamma)\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,np.sum(rewards),(np.sum(rewards)>195.0)))\n",
    "    if np.sum(rewards)>195.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-circular",
   "metadata": {},
   "source": [
    "# Caso del reinforce continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "living-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self,action):\n",
    "        action=(action+1)/2\n",
    "        action*=(self.action_space.high-self.action_space.low)\n",
    "        action+=self.action_space.low\n",
    "        return action\n",
    "    def reverse_action(self,action):\n",
    "        '''\n",
    "        Regresar la acción al estado original en el que se encontraba\n",
    "        '''\n",
    "        action-=self.action_space.low\n",
    "        action/=(self.action_space.high-self.action_space.low)\n",
    "        action=2*action-1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "short-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "digital-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi])).to(device)\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "\n",
    "class Pi(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Pi, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = Pi(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).to(self.device))\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "\n",
    "        eps = torch.randn(mu.size())\n",
    "        # Cálculo de la probabilidad\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).to(self.device)).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
    "\n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).to(self.device)).sum() - (0.0001*entropies[i].to(self.device)).sum()\n",
    "        loss = loss / len(rewards)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-endorsement",
   "metadata": {},
   "source": [
    "# Entrenamiento en el caso continuo, MountainCar Continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ideal-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"MountainCarContinuous-v0\"\n",
    "gamma=0.99\n",
    "hidden_size=128\n",
    "env=NormalizedActions(gym.make(env_name))\n",
    "#env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "#Lo anterior para llevar un monitoreo del entrenamiento de las cosas\n",
    "num_episodes=2000\n",
    "num_steps=300\n",
    "agent=REINFORCE(hidden_size,env.observation_space.shape[0],env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "tender-trout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-de9f381d8013>:59: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  utils.clip_grad_norm(self.model.parameters(), 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: -21.53030062224301 Solved: False\n",
      "Episode: 10 Reward: -18.442836828647568 Solved: False\n",
      "Episode: 20 Reward: -23.25645587279383 Solved: False\n",
      "Episode: 30 Reward: -18.877414309498697 Solved: False\n",
      "Episode: 40 Reward: -18.643845774046664 Solved: False\n",
      "Episode: 50 Reward: -18.061640365677405 Solved: False\n",
      "Episode: 60 Reward: -17.344878157426674 Solved: False\n",
      "Episode: 70 Reward: -15.547342176223921 Solved: False\n",
      "Episode: 80 Reward: -13.197227439207397 Solved: False\n",
      "Episode: 90 Reward: -14.042457991930837 Solved: False\n",
      "Episode: 100 Reward: -10.182006949983064 Solved: False\n",
      "Episode: 110 Reward: -9.806403044352603 Solved: False\n",
      "Episode: 120 Reward: -12.840906211515104 Solved: False\n",
      "Episode: 130 Reward: -11.295072663695077 Solved: False\n",
      "Episode: 140 Reward: -11.493682242199947 Solved: False\n",
      "Episode: 150 Reward: -11.141050188545345 Solved: False\n",
      "Episode: 160 Reward: -9.288206109559809 Solved: False\n",
      "Episode: 170 Reward: -10.000767696831534 Solved: False\n",
      "Episode: 180 Reward: -9.11064424912992 Solved: False\n",
      "Episode: 190 Reward: -7.557890583098143 Solved: False\n",
      "Episode: 200 Reward: -7.473058832052876 Solved: False\n",
      "Episode: 210 Reward: -7.988916222835907 Solved: False\n",
      "Episode: 220 Reward: -7.247064653781883 Solved: False\n",
      "Episode: 230 Reward: -7.501332462920335 Solved: False\n",
      "Episode: 240 Reward: -6.115611825262916 Solved: False\n",
      "Episode: 250 Reward: -5.805080129077025 Solved: False\n",
      "Episode: 260 Reward: -6.299237640253958 Solved: False\n",
      "Episode: 270 Reward: -8.015531232065573 Solved: False\n",
      "Episode: 280 Reward: -7.660135635011186 Solved: False\n",
      "Episode: 290 Reward: -5.677281388847076 Solved: False\n",
      "Episode: 300 Reward: -4.776743936060058 Solved: False\n",
      "Episode: 310 Reward: -4.904302330932443 Solved: False\n",
      "Episode: 320 Reward: -3.763492379597826 Solved: False\n",
      "Episode: 330 Reward: -3.4993559439786823 Solved: False\n",
      "Episode: 340 Reward: -4.027212145396984 Solved: False\n",
      "Episode: 350 Reward: -3.637730074753832 Solved: False\n",
      "Episode: 360 Reward: -3.0474710440023687 Solved: False\n",
      "Episode: 370 Reward: -3.023180827798045 Solved: False\n",
      "Episode: 380 Reward: -2.405971645578848 Solved: False\n",
      "Episode: 390 Reward: -3.078619866021916 Solved: False\n",
      "Episode: 400 Reward: -2.8454098542753994 Solved: False\n",
      "Episode: 410 Reward: -2.4884892390526248 Solved: False\n",
      "Episode: 420 Reward: -2.489009183378805 Solved: False\n",
      "Episode: 430 Reward: -2.499898534513721 Solved: False\n",
      "Episode: 440 Reward: -2.347871008247054 Solved: False\n",
      "Episode: 450 Reward: -2.262774141791988 Solved: False\n",
      "Episode: 460 Reward: -2.049554589485042 Solved: False\n",
      "Episode: 470 Reward: -1.7917340481930848 Solved: False\n",
      "Episode: 480 Reward: -1.8145970819497135 Solved: False\n",
      "Episode: 490 Reward: -1.7199939052339939 Solved: False\n",
      "Episode: 500 Reward: -1.9358839752210568 Solved: False\n",
      "Episode: 510 Reward: -1.7098861153354319 Solved: False\n",
      "Episode: 520 Reward: -2.0334711746443377 Solved: False\n",
      "Episode: 530 Reward: -2.296369530026769 Solved: False\n",
      "Episode: 540 Reward: -1.9576919059453401 Solved: False\n",
      "Episode: 550 Reward: -1.7207384923996576 Solved: False\n",
      "Episode: 560 Reward: -1.859602883853809 Solved: False\n",
      "Episode: 570 Reward: -1.7714053920363217 Solved: False\n",
      "Episode: 580 Reward: -1.738918010859423 Solved: False\n",
      "Episode: 590 Reward: -1.752363579031491 Solved: False\n",
      "Episode: 600 Reward: -1.448199694357247 Solved: False\n",
      "Episode: 610 Reward: -1.4090163580528476 Solved: False\n",
      "Episode: 620 Reward: -1.4873946903477298 Solved: False\n",
      "Episode: 630 Reward: -1.3636352743990332 Solved: False\n",
      "Episode: 640 Reward: -1.3450704009562093 Solved: False\n",
      "Episode: 650 Reward: -1.3061102860628913 Solved: False\n",
      "Episode: 660 Reward: -1.2737476341566765 Solved: False\n",
      "Episode: 670 Reward: -1.3016233420427556 Solved: False\n",
      "Episode: 680 Reward: -1.1054028816248416 Solved: False\n",
      "Episode: 690 Reward: -1.222300973494997 Solved: False\n",
      "Episode: 700 Reward: -1.1038141213994601 Solved: False\n",
      "Episode: 710 Reward: -1.2327992520810436 Solved: False\n",
      "Episode: 720 Reward: -1.1571641162832047 Solved: False\n",
      "Episode: 730 Reward: -0.9968161234326043 Solved: False\n",
      "Episode: 740 Reward: -0.9965035859618233 Solved: False\n",
      "Episode: 750 Reward: -0.9814353058169992 Solved: False\n",
      "Episode: 760 Reward: -1.0137618441609786 Solved: False\n",
      "Episode: 770 Reward: -0.927143723851846 Solved: False\n",
      "Episode: 780 Reward: -0.7950560961002214 Solved: False\n",
      "Episode: 790 Reward: -0.9670594157552944 Solved: False\n",
      "Episode: 800 Reward: -1.0443113718072543 Solved: False\n",
      "Episode: 810 Reward: -1.2123621468906407 Solved: False\n",
      "Episode: 820 Reward: -1.1009709406405885 Solved: False\n",
      "Episode: 830 Reward: -1.0486684004552642 Solved: False\n",
      "Episode: 840 Reward: -0.7971298792624864 Solved: False\n",
      "Episode: 850 Reward: -0.7770933947986027 Solved: False\n",
      "Episode: 860 Reward: -0.635768017383021 Solved: False\n",
      "Episode: 870 Reward: -0.8137825835507748 Solved: False\n",
      "Episode: 880 Reward: -0.6731729268535156 Solved: False\n",
      "Episode: 890 Reward: -1.270020130231833 Solved: False\n",
      "Episode: 900 Reward: -0.7734505427912929 Solved: False\n",
      "Episode: 910 Reward: -0.7496601194698855 Solved: False\n",
      "Episode: 920 Reward: -0.6678179172390302 Solved: False\n",
      "Episode: 930 Reward: -0.6063749042478357 Solved: False\n",
      "Episode: 940 Reward: -0.6235540122579586 Solved: False\n",
      "Episode: 950 Reward: -0.5784243840208856 Solved: False\n",
      "Episode: 960 Reward: -0.593944878467304 Solved: False\n",
      "Episode: 970 Reward: -0.6015356604570787 Solved: False\n",
      "Episode: 980 Reward: -0.6242289075442451 Solved: False\n",
      "Episode: 990 Reward: -0.7200284391598888 Solved: False\n",
      "Episode: 1000 Reward: -0.5554356917748781 Solved: False\n",
      "Episode: 1010 Reward: -0.6433120646225696 Solved: False\n",
      "Episode: 1020 Reward: -0.4935332971479831 Solved: False\n",
      "Episode: 1030 Reward: -0.7467813322967101 Solved: False\n",
      "Episode: 1040 Reward: -0.6805899519620368 Solved: False\n",
      "Episode: 1050 Reward: -0.5443701612864086 Solved: False\n",
      "Episode: 1060 Reward: -0.5263396469320363 Solved: False\n",
      "Episode: 1070 Reward: -0.6782378557797678 Solved: False\n",
      "Episode: 1080 Reward: -0.7577142874392351 Solved: False\n",
      "Episode: 1090 Reward: -0.6671378553565056 Solved: False\n",
      "Episode: 1100 Reward: -0.6504392817777682 Solved: False\n",
      "Episode: 1110 Reward: -0.39917097665157397 Solved: False\n",
      "Episode: 1120 Reward: -0.40162084903307727 Solved: False\n",
      "Episode: 1130 Reward: -0.5331837924068058 Solved: False\n",
      "Episode: 1140 Reward: -0.5032793220988009 Solved: False\n",
      "Episode: 1150 Reward: -0.516352589549929 Solved: False\n",
      "Episode: 1160 Reward: -0.5915606490327221 Solved: False\n",
      "Episode: 1170 Reward: -0.38141645240496547 Solved: False\n",
      "Episode: 1180 Reward: -0.35012349918893193 Solved: False\n",
      "Episode: 1190 Reward: -0.3974141452978202 Solved: False\n",
      "Episode: 1200 Reward: -0.3431115587536052 Solved: False\n",
      "Episode: 1210 Reward: -0.4226109987707076 Solved: False\n",
      "Episode: 1220 Reward: -0.5985577131862958 Solved: False\n",
      "Episode: 1230 Reward: -0.7295978934382057 Solved: False\n",
      "Episode: 1240 Reward: -0.6473998173624385 Solved: False\n",
      "Episode: 1250 Reward: -0.7630183187538015 Solved: False\n",
      "Episode: 1260 Reward: -0.3739257773227039 Solved: False\n",
      "Episode: 1270 Reward: -0.3857746666530439 Solved: False\n",
      "Episode: 1280 Reward: -0.3562168719738985 Solved: False\n",
      "Episode: 1290 Reward: -0.37006851285721326 Solved: False\n",
      "Episode: 1300 Reward: -0.30165587075705 Solved: False\n",
      "Episode: 1310 Reward: -0.3147894035761152 Solved: False\n",
      "Episode: 1320 Reward: -0.34380033336307003 Solved: False\n",
      "Episode: 1330 Reward: -0.3649345052133054 Solved: False\n",
      "Episode: 1340 Reward: -0.3488106964880363 Solved: False\n",
      "Episode: 1350 Reward: -0.38724791813373616 Solved: False\n",
      "Episode: 1360 Reward: -0.37920450472194606 Solved: False\n",
      "Episode: 1370 Reward: -0.3080785608395523 Solved: False\n",
      "Episode: 1380 Reward: -0.3377760183164359 Solved: False\n",
      "Episode: 1390 Reward: -0.3072206466275695 Solved: False\n",
      "Episode: 1400 Reward: -0.34699846872956897 Solved: False\n",
      "Episode: 1410 Reward: -0.27208567016541063 Solved: False\n",
      "Episode: 1420 Reward: -0.30457407325432584 Solved: False\n",
      "Episode: 1430 Reward: -0.27223566156650564 Solved: False\n",
      "Episode: 1440 Reward: -0.29781321822369056 Solved: False\n",
      "Episode: 1450 Reward: -0.3099527726902167 Solved: False\n",
      "Episode: 1460 Reward: -0.6164923910736584 Solved: False\n",
      "Episode: 1470 Reward: -0.6382692434142527 Solved: False\n",
      "Episode: 1480 Reward: -0.7115084002585441 Solved: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1490 Reward: -0.847191070951713 Solved: False\n",
      "Episode: 1500 Reward: -0.6323860259255594 Solved: False\n",
      "Episode: 1510 Reward: -1.4765843479124223 Solved: False\n",
      "Episode: 1520 Reward: -0.8400208729946925 Solved: False\n",
      "Episode: 1530 Reward: -1.2137844002792404 Solved: False\n",
      "Episode: 1540 Reward: -0.39234168875051023 Solved: False\n",
      "Episode: 1550 Reward: -0.17940806536787618 Solved: False\n",
      "Episode: 1560 Reward: -0.2198252265990327 Solved: False\n",
      "Episode: 1570 Reward: -0.20198192292178307 Solved: False\n",
      "Episode: 1580 Reward: -0.1904529071012341 Solved: False\n",
      "Episode: 1590 Reward: -0.1933106280967795 Solved: False\n",
      "Episode: 1600 Reward: -0.23789964220708948 Solved: False\n",
      "Episode: 1610 Reward: -0.30482488483199754 Solved: False\n",
      "Episode: 1620 Reward: -1.252616409809096 Solved: False\n",
      "Episode: 1630 Reward: -0.7092384351960124 Solved: False\n",
      "Episode: 1640 Reward: -0.31732735232961995 Solved: False\n",
      "Episode: 1650 Reward: -0.8905022929070437 Solved: False\n",
      "Episode: 1660 Reward: -0.9281521388123125 Solved: False\n",
      "Episode: 1670 Reward: -0.2804750155730933 Solved: False\n",
      "Episode: 1680 Reward: -0.16171029189674613 Solved: False\n",
      "Episode: 1690 Reward: -0.2891052690876492 Solved: False\n",
      "Episode: 1700 Reward: -0.2944506366751479 Solved: False\n",
      "Episode: 1710 Reward: -0.2389186935529107 Solved: False\n",
      "Episode: 1720 Reward: -0.15183718149655726 Solved: False\n",
      "Episode: 1730 Reward: -0.12737627989475087 Solved: False\n",
      "Episode: 1740 Reward: -0.1295411227979397 Solved: False\n",
      "Episode: 1750 Reward: -0.1229443958717166 Solved: False\n",
      "Episode: 1760 Reward: -0.1392075422146377 Solved: False\n",
      "Episode: 1770 Reward: -0.13724747173057958 Solved: False\n",
      "Episode: 1780 Reward: -0.14351964038399317 Solved: False\n",
      "Episode: 1790 Reward: -0.1288260500383551 Solved: False\n",
      "Episode: 1800 Reward: -0.12300714397576903 Solved: False\n",
      "Episode: 1810 Reward: -0.14436666670903547 Solved: False\n",
      "Episode: 1820 Reward: -0.12419768678442049 Solved: False\n",
      "Episode: 1830 Reward: -0.12659696892761013 Solved: False\n",
      "Episode: 1840 Reward: -0.15984397761636232 Solved: False\n",
      "Episode: 1850 Reward: -0.09330091882212807 Solved: False\n",
      "Episode: 1860 Reward: -0.14316788526485136 Solved: False\n",
      "Episode: 1870 Reward: -0.10495592756250041 Solved: False\n",
      "Episode: 1880 Reward: -0.109646941534292 Solved: False\n",
      "Episode: 1890 Reward: -0.10779770240384073 Solved: False\n",
      "Episode: 1900 Reward: -0.13092040663643728 Solved: False\n",
      "Episode: 1910 Reward: -0.10940608335170268 Solved: False\n",
      "Episode: 1920 Reward: -0.1115543766873774 Solved: False\n",
      "Episode: 1930 Reward: -0.1261272873336715 Solved: False\n",
      "Episode: 1940 Reward: -0.10625104378177747 Solved: False\n",
      "Episode: 1950 Reward: -0.10428981542993691 Solved: False\n",
      "Episode: 1960 Reward: -0.1093283698593421 Solved: False\n",
      "Episode: 1970 Reward: -0.11471444561095367 Solved: False\n",
      "Episode: 1980 Reward: -0.15112902630688263 Solved: False\n",
      "Episode: 1990 Reward: -0.11211567010016843 Solved: False\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state=torch.Tensor([env.reset()])\n",
    "    log_probs=[]\n",
    "    entropys=[]\n",
    "    rewards=[]\n",
    "    for t in range(num_steps):\n",
    "        action,log_prob,entropy=agent.select_action(state)\n",
    "        action=action.cpu()\n",
    "        next_state,reward,done,_=env.step(action.numpy()[0]) #El step necesita una acción para llegar al siguiente estado y sacar recompensa\n",
    "        #Actualizamos la entropia y las recompensas y las log probs\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropys.append(entropy)\n",
    "        state=torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_parameters(rewards,log_probs,entropys,gamma)\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,np.sum(rewards),(np.sum(rewards)>11.0)))\n",
    "    if np.sum(rewards)>11.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-plumbing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
