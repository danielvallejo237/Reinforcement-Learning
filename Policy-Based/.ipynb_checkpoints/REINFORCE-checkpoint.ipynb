{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functioning-angle",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo de reinforce en el caso discreto y continuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-joseph",
   "metadata": {},
   "source": [
    "# Caso discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "certified-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import pdb\n",
    "from torch.autograd import Variable\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-documentary",
   "metadata": {},
   "source": [
    "El primer paso es construir la red de política o policy network, generalmente consta de capas densas en donde se da la dimensión de entrada y la dimesión de salida de la red, calculan probabilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boxed-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(nn.Module):\n",
    "    def __init__(self,hidden_size,num_inputs,action_space):\n",
    "        super(Pi,self).__init__()\n",
    "        self.action_space=action_space\n",
    "        out_dim=action_space.n\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_inputs=num_inputs\n",
    "        layers=[\n",
    "            nn.Linear(self.num_inputs,self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size,out_dim),\n",
    "            nn.Softmax(),  \n",
    "        ]\n",
    "        '''\n",
    "        En el modelo descrito en el libro usan Categorical para sacar las probailidades de las\n",
    "        acciones, en este caso usamos la función Soft Max\n",
    "        Pi en este caso representa nuestra Policy Network\n",
    "        '''\n",
    "        self.model=nn.Sequential(*layers)\n",
    "        self.train()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.model(x)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self,action_space,hidden_size,num_inputs):\n",
    "        self.action_space=action_space\n",
    "        self.model=Pi(hidden_size,num_inputs,action_space)\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model=self.model.to(self.device) #Mandamos el modelo a cuda para entrenar\n",
    "        self.model.train()\n",
    "        self.optimizer=optim.Adam(self.model.parameters(),lr=1e-3) #Estamos optimizando el peso de la política\n",
    "        #Usamos como optimizador descenso de gradiente estocástico\n",
    "    def select_action(self,state):\n",
    "        probs=self.model(Variable(state).to(self.device))\n",
    "        action=probs.multinomial(1).data\n",
    "        prob=probs[:,action[0,0]].view(1,-1)\n",
    "        log_prob=prob.log()\n",
    "        entropy=-(probs*probs.log()).sum()\n",
    "        return action[0], log_prob, entropy\n",
    "    def update_parameters(self,rewards,log_probs,entropy,gamma):\n",
    "        '''\n",
    "        En este caso queremos actualizar el Policy Gradient el cual necesita de las recompesas, la log\n",
    "        probabilidad y el factor de descuento gamma\n",
    "        '''\n",
    "        R=torch.zeros(1,1)\n",
    "        loss=0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            R=rewards[t]+gamma*R\n",
    "            loss=loss- (log_probs[t]*(Variable(R).expand_as(log_probs[t])).to(self.device)).sum()-(0.00001*entropy[t].to(self.device)).sum()\n",
    "        loss=loss/len(rewards)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-victoria",
   "metadata": {},
   "source": [
    "Recordemos lo siguiente:\n",
    "\n",
    "El \"Policy gradient\", se define como $\\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\sum_{t=0}^{T}R_{t}(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$\n",
    "Sin embargo usando la actualización por muestreo de Monte Carlo, la actualización toma la siguiete forma\n",
    "$$ \\nabla_{\\theta} J(\\pi_{\\theta})= \\nabla_{\\theta} J(\\pi_{\\theta})+ R(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_{t}|s_{t})$$ para algun tiempo $t \\in \\{0,...,T\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-means",
   "metadata": {},
   "source": [
    "# Entrenamiento principal en ambientes de gym, CartPole V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"CartPole-v0\"\n",
    "gamma=0.99\n",
    "hidden_size=128\n",
    "env=gym.make(env_name)\n",
    "#env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "#Lo anterior para llevar un monitoreo del entrenamiento de las cosas\n",
    "num_episodes=1000\n",
    "num_steps=200\n",
    "agent=REINFORCE(env.action_space,hidden_size,env.observation_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "political-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gothic-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 20.0 Solved: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel.vallejo/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "<ipython-input-4-864ce215360d>:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  utils.clip_grad_norm(self.model.parameters(),40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10 Reward: 17.0 Solved: False\n",
      "Episode: 20 Reward: 35.0 Solved: False\n",
      "Episode: 30 Reward: 34.0 Solved: False\n",
      "Episode: 40 Reward: 49.0 Solved: False\n",
      "Episode: 50 Reward: 64.0 Solved: False\n",
      "Episode: 60 Reward: 18.0 Solved: False\n",
      "Episode: 70 Reward: 30.0 Solved: False\n",
      "Episode: 80 Reward: 34.0 Solved: False\n",
      "Episode: 90 Reward: 22.0 Solved: False\n",
      "Episode: 100 Reward: 15.0 Solved: False\n",
      "Episode: 110 Reward: 56.0 Solved: False\n",
      "Episode: 120 Reward: 21.0 Solved: False\n",
      "Episode: 130 Reward: 67.0 Solved: False\n",
      "Episode: 140 Reward: 13.0 Solved: False\n",
      "Episode: 150 Reward: 50.0 Solved: False\n",
      "Episode: 160 Reward: 82.0 Solved: False\n",
      "Episode: 170 Reward: 31.0 Solved: False\n",
      "Episode: 180 Reward: 37.0 Solved: False\n",
      "Episode: 190 Reward: 32.0 Solved: False\n",
      "Episode: 200 Reward: 74.0 Solved: False\n",
      "Episode: 210 Reward: 35.0 Solved: False\n",
      "Episode: 220 Reward: 36.0 Solved: False\n",
      "Episode: 230 Reward: 58.0 Solved: False\n",
      "Episode: 240 Reward: 49.0 Solved: False\n",
      "Episode: 250 Reward: 137.0 Solved: False\n",
      "Episode: 260 Reward: 67.0 Solved: False\n",
      "Episode: 270 Reward: 72.0 Solved: False\n",
      "Episode: 280 Reward: 24.0 Solved: False\n",
      "Episode: 290 Reward: 34.0 Solved: False\n",
      "Episode: 300 Reward: 82.0 Solved: False\n",
      "Episode: 310 Reward: 86.0 Solved: False\n",
      "Episode: 320 Reward: 181.0 Solved: False\n",
      "Episode: 330 Reward: 114.0 Solved: False\n",
      "Episode: 340 Reward: 103.0 Solved: False\n",
      "Episode: 350 Reward: 47.0 Solved: False\n",
      "Episode: 360 Reward: 109.0 Solved: False\n",
      "Episode: 370 Reward: 32.0 Solved: False\n",
      "Episode: 380 Reward: 82.0 Solved: False\n",
      "Episode: 390 Reward: 128.0 Solved: False\n",
      "Solved after 391 episodes\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state=torch.Tensor([env.reset()])\n",
    "    log_probs=[]\n",
    "    entropys=[]\n",
    "    rewards=[]\n",
    "    for t in range(num_steps):\n",
    "        action,log_prob,entropy=agent.select_action(state)\n",
    "        action=action.cpu()\n",
    "        next_state,reward,done,_=env.step(action.numpy()[0]) #El step necesita una acción para llegar al siguiente estado y sacar recompensa\n",
    "        #Actualizamos la entropia y las recompensas y las log probs\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropys.append(entropy)\n",
    "        state=torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_parameters(rewards,log_probs,entropys,gamma)\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,np.sum(rewards),(np.sum(rewards)>195.0)))\n",
    "    if np.sum(rewards)>195.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-circular",
   "metadata": {},
   "source": [
    "# Caso del reinforce continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "living-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self,action):\n",
    "        action=(action+1)/2\n",
    "        action*=(self.action_space.high-self.action_space.low)\n",
    "        action+=self.action_space.low\n",
    "        return action\n",
    "    def reverse_action(self,action):\n",
    "        '''\n",
    "        Regresar la acción al estado original en el que se encontraba\n",
    "        '''\n",
    "        action-=self.action_space.low\n",
    "        action/=(self.action_space.high-self.action_space.low)\n",
    "        action=2*action-1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "auburn-literacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "digital-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi])).to(device)\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "\n",
    "class Pi(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Pi, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = Pi(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).to(self.device))\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "\n",
    "        eps = torch.randn(mu.size())\n",
    "        # Cálculo de la probabilidad\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).to(self.device)).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
    "\n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).to(self.device)).sum() - (0.0001*entropies[i].to(self.device)).sum()\n",
    "        loss = loss / len(rewards)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-defensive",
   "metadata": {},
   "source": [
    "# Entrenamiento en el caso continuo, MountainCar Continuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "organic-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"MountainCarContinuous-v0\"\n",
    "gamma=0.99\n",
    "hidden_size=128\n",
    "env=NormalizedActions(gym.make(env_name))\n",
    "#env = wrappers.Monitor(env, '/tmp/{}-experiment'.format(env_name), force=True)\n",
    "#Lo anterior para llevar un monitoreo del entrenamiento de las cosas\n",
    "num_episodes=2000\n",
    "num_steps=300\n",
    "agent=REINFORCE(hidden_size,env.observation_space.shape[0],env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "suspended-programmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-de9f381d8013>:59: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  utils.clip_grad_norm(self.model.parameters(), 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: -16.586713615896453 Solved: False\n",
      "Episode: 10 Reward: -16.598472237582854 Solved: False\n",
      "Episode: 20 Reward: -18.784567350395022 Solved: False\n",
      "Episode: 30 Reward: -14.02263920954978 Solved: False\n",
      "Episode: 40 Reward: -14.220293400629961 Solved: False\n",
      "Episode: 50 Reward: -11.574428735422568 Solved: False\n",
      "Episode: 60 Reward: -11.420059764839971 Solved: False\n",
      "Episode: 70 Reward: -12.005607882630095 Solved: False\n",
      "Episode: 80 Reward: -9.73143758602627 Solved: False\n",
      "Episode: 90 Reward: -11.29716722690776 Solved: False\n",
      "Episode: 100 Reward: -9.719311681161184 Solved: False\n",
      "Episode: 110 Reward: -8.18794257238563 Solved: False\n",
      "Episode: 120 Reward: -6.8914034389440175 Solved: False\n",
      "Episode: 130 Reward: -5.871157179465502 Solved: False\n",
      "Episode: 140 Reward: -6.892526454205981 Solved: False\n",
      "Episode: 150 Reward: -6.925948872697424 Solved: False\n",
      "Episode: 160 Reward: -6.70697605793322 Solved: False\n",
      "Episode: 170 Reward: -6.029002478268797 Solved: False\n",
      "Episode: 180 Reward: -5.604776311536094 Solved: False\n",
      "Episode: 190 Reward: -6.610804028522973 Solved: False\n",
      "Episode: 200 Reward: -5.556999173785458 Solved: False\n",
      "Episode: 210 Reward: -6.467207172707584 Solved: False\n",
      "Episode: 220 Reward: -5.073802443074527 Solved: False\n",
      "Episode: 230 Reward: -4.609158036006628 Solved: False\n",
      "Episode: 240 Reward: -4.697220435375343 Solved: False\n",
      "Episode: 250 Reward: -5.098753818703737 Solved: False\n",
      "Episode: 260 Reward: -4.120983732914218 Solved: False\n",
      "Episode: 270 Reward: -4.64408579349441 Solved: False\n",
      "Episode: 280 Reward: -5.139708062419684 Solved: False\n",
      "Episode: 290 Reward: -5.289135613062331 Solved: False\n",
      "Episode: 300 Reward: -4.999441216851773 Solved: False\n",
      "Episode: 310 Reward: -4.507920608469226 Solved: False\n",
      "Episode: 320 Reward: -3.4509324399163512 Solved: False\n",
      "Episode: 330 Reward: -3.079906328349287 Solved: False\n",
      "Episode: 340 Reward: -3.2825743562960987 Solved: False\n",
      "Episode: 350 Reward: -3.0310281938632477 Solved: False\n",
      "Episode: 360 Reward: -2.8967577071970445 Solved: False\n",
      "Episode: 370 Reward: -2.7490697854749246 Solved: False\n",
      "Episode: 380 Reward: -2.9049446212936347 Solved: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-65c0040347ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#El step necesita una acción para llegar al siguiente estado y sacar recompensa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-de9f381d8013>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-de9f381d8013>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_sq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state=torch.Tensor([env.reset()])\n",
    "    log_probs=[]\n",
    "    entropys=[]\n",
    "    rewards=[]\n",
    "    for t in range(num_steps):\n",
    "        action,log_prob,entropy=agent.select_action(state)\n",
    "        action=action.cpu()\n",
    "        next_state,reward,done,_=env.step(action.numpy()[0]) #El step necesita una acción para llegar al siguiente estado y sacar recompensa\n",
    "        #Actualizamos la entropia y las recompensas y las log probs\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        entropys.append(entropy)\n",
    "        state=torch.Tensor([next_state])\n",
    "        if done:\n",
    "            break\n",
    "    agent.update_parameters(rewards,log_probs,entropys,gamma)\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,np.sum(rewards),(np.sum(rewards)>11.0)))\n",
    "    if np.sum(rewards)>11.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
