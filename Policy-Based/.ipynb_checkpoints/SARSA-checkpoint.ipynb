{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baking-mapping",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo de SARSA \n",
    "## (State-Action-Reward-State-Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-measure",
   "metadata": {},
   "source": [
    "Implementación del algorimo de SARSA usando formato tabular y actualizando los valores de acuerdo a la siguiente fórmula\n",
    "$$\n",
    "Q^{\\pi}(s,a)=r+\\gammaQ^{\\pi}(s',a')\n",
    "$$\n",
    "Donde $s'$,$a'$ son los estados siguientes y la acción siguiente tomada.\n",
    "Se usa la técnica de Epsilon greedy para que el agente explore todo el ambiente en un inicio y despúes aprenda de sus experiencias reduciendo esa aleatoriedad del epsilon\n",
    "Ese valor $\\epsilon$ decae conforme el tiempo del episodio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "successful-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "civil-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consideremos un ambiente discreto\n",
    "env_name='CartPole-v0'\n",
    "env=gym.make(env_name)\n",
    "epsilon_start=0.99\n",
    "epsilon_end=0.01\n",
    "max_steps=1000\n",
    "alpha=0.99\n",
    "gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sharp-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero haremos una implementación tabular de SARSA\n",
    "class QTable:\n",
    "    def __init__(self,action_space,observation_space):\n",
    "        self.device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.action_space=action_space\n",
    "        self.observation_space=observation_space\n",
    "        self.observation_size=observation_space.shape[0]\n",
    "        self.action_size=action_space.n #Datos necesarios para el caso de SARSA\n",
    "        self.q_table=torch.zeros(self.observation_size,self.action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "toxic-advance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla=QTable(env.action_space,env.observation_space)\n",
    "tabla.q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-model",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo, calculo de los valores Q en forma de tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "pending-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 10.0 Solved: False\n",
      "Episode: 10 Reward: 31.0 Solved: False\n",
      "Episode: 20 Reward: 14.0 Solved: False\n",
      "Episode: 30 Reward: 41.0 Solved: False\n",
      "Episode: 40 Reward: 10.0 Solved: False\n",
      "Episode: 50 Reward: 26.0 Solved: False\n",
      "Episode: 60 Reward: 11.0 Solved: False\n",
      "Episode: 70 Reward: 22.0 Solved: False\n",
      "Episode: 80 Reward: 11.0 Solved: False\n",
      "Episode: 90 Reward: 23.0 Solved: False\n",
      "Episode: 100 Reward: 11.0 Solved: False\n",
      "Episode: 110 Reward: 25.0 Solved: False\n",
      "Episode: 120 Reward: 12.0 Solved: False\n",
      "Episode: 130 Reward: 51.0 Solved: False\n",
      "Episode: 140 Reward: 28.0 Solved: False\n",
      "Episode: 150 Reward: 10.0 Solved: False\n",
      "Episode: 160 Reward: 24.0 Solved: False\n",
      "Episode: 170 Reward: 79.0 Solved: False\n",
      "Episode: 180 Reward: 9.0 Solved: False\n",
      "Episode: 190 Reward: 19.0 Solved: False\n",
      "Episode: 200 Reward: 31.0 Solved: False\n",
      "Episode: 210 Reward: 22.0 Solved: False\n",
      "Episode: 220 Reward: 46.0 Solved: False\n",
      "Episode: 230 Reward: 23.0 Solved: False\n",
      "Episode: 240 Reward: 10.0 Solved: False\n",
      "Episode: 250 Reward: 9.0 Solved: False\n",
      "Episode: 260 Reward: 39.0 Solved: False\n",
      "Episode: 270 Reward: 28.0 Solved: False\n",
      "Episode: 280 Reward: 25.0 Solved: False\n",
      "Episode: 290 Reward: 12.0 Solved: False\n",
      "Episode: 300 Reward: 37.0 Solved: False\n",
      "Episode: 310 Reward: 38.0 Solved: False\n",
      "Episode: 320 Reward: 23.0 Solved: False\n",
      "Episode: 330 Reward: 11.0 Solved: False\n",
      "Episode: 340 Reward: 35.0 Solved: False\n",
      "Episode: 350 Reward: 33.0 Solved: False\n",
      "Episode: 360 Reward: 10.0 Solved: False\n",
      "Episode: 370 Reward: 32.0 Solved: False\n",
      "Episode: 380 Reward: 10.0 Solved: False\n",
      "Episode: 390 Reward: 32.0 Solved: False\n",
      "Episode: 400 Reward: 34.0 Solved: False\n",
      "Episode: 410 Reward: 22.0 Solved: False\n",
      "Episode: 420 Reward: 11.0 Solved: False\n",
      "Episode: 430 Reward: 29.0 Solved: False\n",
      "Episode: 440 Reward: 10.0 Solved: False\n",
      "Episode: 450 Reward: 29.0 Solved: False\n",
      "Episode: 460 Reward: 10.0 Solved: False\n",
      "Episode: 470 Reward: 13.0 Solved: False\n",
      "Episode: 480 Reward: 33.0 Solved: False\n",
      "Episode: 490 Reward: 11.0 Solved: False\n",
      "Episode: 500 Reward: 34.0 Solved: False\n",
      "Episode: 510 Reward: 12.0 Solved: False\n",
      "Episode: 520 Reward: 9.0 Solved: False\n",
      "Episode: 530 Reward: 15.0 Solved: False\n",
      "Episode: 540 Reward: 10.0 Solved: False\n",
      "Episode: 550 Reward: 26.0 Solved: False\n",
      "Episode: 560 Reward: 45.0 Solved: False\n",
      "Episode: 570 Reward: 24.0 Solved: False\n",
      "Episode: 580 Reward: 42.0 Solved: False\n",
      "Episode: 590 Reward: 27.0 Solved: False\n",
      "Episode: 600 Reward: 26.0 Solved: False\n",
      "Episode: 610 Reward: 40.0 Solved: False\n",
      "Episode: 620 Reward: 10.0 Solved: False\n",
      "Episode: 630 Reward: 11.0 Solved: False\n",
      "Episode: 640 Reward: 36.0 Solved: False\n",
      "Episode: 650 Reward: 10.0 Solved: False\n",
      "Episode: 660 Reward: 32.0 Solved: False\n",
      "Episode: 670 Reward: 9.0 Solved: False\n",
      "Episode: 680 Reward: 10.0 Solved: False\n",
      "Episode: 690 Reward: 25.0 Solved: False\n",
      "Episode: 700 Reward: 21.0 Solved: False\n",
      "Episode: 710 Reward: 30.0 Solved: False\n",
      "Episode: 720 Reward: 18.0 Solved: False\n",
      "Episode: 730 Reward: 41.0 Solved: False\n",
      "Episode: 740 Reward: 39.0 Solved: False\n",
      "Episode: 750 Reward: 49.0 Solved: False\n",
      "Episode: 760 Reward: 39.0 Solved: False\n",
      "Episode: 770 Reward: 77.0 Solved: False\n",
      "Episode: 780 Reward: 10.0 Solved: False\n",
      "Episode: 790 Reward: 12.0 Solved: False\n",
      "Episode: 800 Reward: 35.0 Solved: False\n",
      "Episode: 810 Reward: 10.0 Solved: False\n",
      "Episode: 820 Reward: 10.0 Solved: False\n",
      "Episode: 830 Reward: 50.0 Solved: False\n",
      "Episode: 840 Reward: 52.0 Solved: False\n",
      "Episode: 850 Reward: 45.0 Solved: False\n",
      "Episode: 860 Reward: 36.0 Solved: False\n",
      "Episode: 870 Reward: 31.0 Solved: False\n",
      "Episode: 880 Reward: 31.0 Solved: False\n",
      "Episode: 890 Reward: 39.0 Solved: False\n",
      "Episode: 900 Reward: 24.0 Solved: False\n",
      "Episode: 910 Reward: 24.0 Solved: False\n",
      "Episode: 920 Reward: 77.0 Solved: False\n",
      "Episode: 930 Reward: 13.0 Solved: False\n",
      "Episode: 940 Reward: 25.0 Solved: False\n",
      "Episode: 950 Reward: 9.0 Solved: False\n",
      "Episode: 960 Reward: 25.0 Solved: False\n",
      "Episode: 970 Reward: 9.0 Solved: False\n",
      "Episode: 980 Reward: 10.0 Solved: False\n",
      "Episode: 990 Reward: 58.0 Solved: False\n"
     ]
    }
   ],
   "source": [
    "for episode in range(max_steps):\n",
    "    done=False\n",
    "    state=env.reset()\n",
    "    next_state=state\n",
    "    total_reward,reward,counter=0,0,0\n",
    "    action=env.action_space.sample()\n",
    "    next_action=action\n",
    "    counter=1;\n",
    "    while not done:\n",
    "        next_state,reward,done,_=env.step(action)\n",
    "        epsilon=max(epsilon_end,epsilon_start*(1/100**counter)) #Decaimiento lineal del epsilon\n",
    "        if random.random() >epsilon:\n",
    "            next_action=tabla.q_table.max(dim=1)[1][0].item()\n",
    "            #print(next_action)\n",
    "        else:\n",
    "            next_action=env.action_space.sample() #Epsilon greedy \n",
    "        tabla.q_table[state,action]+=alpha*(reward+gamma*(tabla.q_table[next_state,next_action]-tabla.q_table[state,action]))\n",
    "        state=next_state\n",
    "        action=next_action\n",
    "        total_reward+=reward \n",
    "        counter+=1\n",
    "    if episode % 10 ==0:\n",
    "        print(\"Episode: {} Reward: {} Solved: {}\".format(episode,total_reward,(total_reward>195.0)))\n",
    "    if total_reward>195.0:\n",
    "        print(\"Solved after {} episodes\".format(episode))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-coordinator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
